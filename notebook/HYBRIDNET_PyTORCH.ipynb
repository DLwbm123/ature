{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/akhanal1/Spring2018/pl-env/bin/python3.5\n",
    "# Torch imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ak/PycharmProjects/ature')\n",
    "os.chdir('/home/ak/PycharmProjects/ature')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from neuralnet.utils.weights_utils import initialize_weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from commons.segmentation import AtureTest\n",
    "from commons.IMAGE import SegmentedImage\n",
    "\n",
    "\n",
    "from utils import img_utils as imgutil\n",
    "from commons.IMAGE import Image\n",
    "from neuralnet.hybridnet.hybridnet_trainer import HybridNNTrainer\n",
    "from neuralnet.hybridnet.hybridnet_dataloader import PatchesGenerator\n",
    "import neuralnet.utils.measurements as mnt\n",
    "import neuralnet.utils.data_utils as nndutils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define folders. Create if needed.\n",
    "sep = os.sep\n",
    "Dirs = {}\n",
    "Dirs['checkpoint']   = 'assests' +sep+ 'nnet_models'\n",
    "Dirs['data']      = 'data'+sep+'DRIVE'+sep+'training'\n",
    "Dirs['images']    = Dirs['data'] +sep+ 'images'\n",
    "Dirs['mask']      = Dirs['data'] +sep+ 'mask'\n",
    "Dirs['truth']     = Dirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "TestDirs = {}\n",
    "TestDirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "TestDirs['images']    = TestDirs['data'] +sep+ 'test_images'\n",
    "TestDirs['mask']      = TestDirs['data'] +sep+ 'mask'\n",
    "TestDirs['truth']     = TestDirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "ValidationDirs = {}\n",
    "ValidationDirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "ValidationDirs['images']    = ValidationDirs['data'] +sep+ 'validation_images'\n",
    "ValidationDirs['mask']      = ValidationDirs['data'] +sep+ 'mask'\n",
    "ValidationDirs['truth']     = ValidationDirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "for k, folder in Dirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "for k, folder in TestDirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "for k, folder in ValidationDirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def get_mask_file(file_name): \n",
    "    return file_name.split('_')[0] + '_training_mask.gif'\n",
    "\n",
    "def get_ground_truth_file(file_name): \n",
    "    return file_name.split('_')[0] + '_manual1.gif'\n",
    "\n",
    "def get_mask_file_test(file_name): \n",
    "    return file_name.split('_')[0] + '_test_mask.gif'\n",
    "\n",
    "classes = { 'background': 0, 'vessel': 1,}\n",
    "batch_size = 4\n",
    "num_classes = len(classes)\n",
    "epochs = 5\n",
    "num_rows, num_cols = 52, 572 #height by width of image of training img\n",
    "use_gpu = False\n",
    "pixel_offset = 5\n",
    "#### Images to train/validate per epoch ####\n",
    "train_size = 2000\n",
    "validation_size = 200\n",
    "checkpoint_file = 'PytorchCheckpointHybridrow64Smart1DeepBNOOOOOAUGMENT.nn.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_width, input_height, channels):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.width = input_width\n",
    "        self.height = input_height\n",
    "        print(self.width, self.height)\n",
    "    \n",
    "        self.kern_size = 3\n",
    "        self.kern_stride = 1\n",
    "        self.kern_padding = 1\n",
    "        self.conv1 = nn.Conv2d(self.channels, 64, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        self.width = self._get_output_size(self.width, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        self.height = self._get_output_size(self.height, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        print(self.width, self.height)\n",
    "        \n",
    "        self.kern_size = 3\n",
    "        self.kern_stride = 1    \n",
    "        self.kern_padding = 1\n",
    "        self.conv2 = nn.Conv2d(64, 64, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.width = self._get_output_size(self.width, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        self.height = self._get_output_size(self.height, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        print(self.width, self.height)\n",
    "        \n",
    "        self.kern_size = 3\n",
    "        self.kern_stride = 1      \n",
    "        self.kern_padding = 1\n",
    "        self.conv3 = nn.Conv2d(64, 128, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self.conv3_bn = nn.BatchNorm2d(128)\n",
    "        self.width = self._get_output_size(self.width, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        self.height = self._get_output_size(self.height, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        print(self.width, self.height)\n",
    "        \n",
    "        self.kern_size = 3\n",
    "        self.kern_stride = 1      \n",
    "        self.kern_padding = 1\n",
    "        self.conv4 = nn.Conv2d(128, 64, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self.conv4_bn = nn.BatchNorm2d(64)\n",
    "        self.width = self._get_output_size(self.width, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        self.height = self._get_output_size(self.height, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        print(self.width, self.height)\n",
    "        \n",
    "        self.kern_size = 1\n",
    "        self.kern_stride = 1      \n",
    "        self.kern_padding = 0\n",
    "        self.out = nn.Conv2d(64, num_classes, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self.width = self._get_output_size(self.width, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        self.height = self._get_output_size(self.height, self.kern_size, self.kern_padding, self.kern_stride)\n",
    "        initialize_weights(self)\n",
    "        print(self.width, self.height)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_bn(self.conv1(x)))\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x = F.relu(self.conv4_bn(self.conv4(x)))\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def _get_output_size(self, w, f, p, s):       \n",
    "        return ((w - f + 2 * p) / s) + 1\n",
    "\n",
    "\n",
    "channels = 1\n",
    "net = Net(num_rows, num_cols, channels)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = PatchesGenerator(Dirs=Dirs, train_image_size=(num_rows, num_cols), \n",
    "                                 transform=transform,\n",
    "                                 fget_mask=get_mask_file, \n",
    "                                 fget_truth=get_ground_truth_file, pixel_offset=pixel_offset, mode='train') \n",
    "\n",
    "train_size = trainset.__len__() if train_size is None else train_size\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=0, \n",
    "                                          sampler=WeightedRandomSampler(np.ones(trainset.__len__()), train_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = PatchesGenerator(Dirs=ValidationDirs, train_image_size=(num_rows, num_cols),  \n",
    "                                       transform=transform,\n",
    "                                       fget_mask=get_mask_file_test, \n",
    "                                       fget_truth=get_ground_truth_file, pixel_offset=pixel_offset, mode='train') \n",
    "\n",
    "validation_size = validation_set.__len__() if validation_size is None else validation_size\n",
    "validationloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, \n",
    "                                            shuffle=False, num_workers=0,\n",
    "                                            sampler=WeightedRandomSampler(np.ones(validation_set.__len__()), \n",
    "                                                                          validation_size, replacement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HybridNNTrainer(model=net, checkpoint_dir=Dirs['checkpoint'], checkpoint_file=checkpoint_file, \n",
    "                          to_tensorboard=True)\n",
    "# trainer.resume_from_checkpoint()\n",
    "trainer.train(optimizer=optimizer, dataloader=trainloader, epochs=epochs, use_gpu=use_gpu, \n",
    "              validationloader=validationloader, force_checkpoint=True, log_frequency=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "testset = PatchesGenerator(Dirs=TestDirs, train_image_size=(num_rows, num_cols), \n",
    "                                transform=transform_test,\n",
    "                                fget_mask=get_mask_file_test, \n",
    "                                fget_truth=get_ground_truth_file,\n",
    "                                 pixel_offset=num_rows, mode='eval') \n",
    "\n",
    "sampler=WeightedRandomSampler(np.ones(testset.__len__()), 100, replacement=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=2, \n",
    "                                          shuffle=False, num_workers=0, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IJs, scores, y_pred, y_true = trainer.evaluate(dataloader=testloader, use_gpu=use_gpu, force_checkpoint=False)\n",
    "# mnt.plot_confusion_matrix(y_pred=y_pred, y_true=y_true, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve throughout the image to generate segmented image based on trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.array(y_pred.reshape(-1, 565)*255, dtype=np.uint8)\n",
    "IMG.fromarray(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.array(y_pred.reshape(-1, 565)*255, dtype=np.uint8)\n",
    "IMG.fromarray(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG.fromarray(np.array(y_pred.reshape(584, 54/6)*255, dtype=np.uint8))\n",
    "i = np.array(y_true.reshape(-1, 565)*255, dtype=np.uint8)\n",
    "IMG.fromarray(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = scores[:,0,:,:]\n",
    "i = np.array(np.exp(k).reshape(-1, 565)*255, dtype=np.uint8)\n",
    "IMG.fromarray(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = scores[:,1,:,:]\n",
    "i = np.array(np.exp(k).reshape(-1, 565)*255, dtype=np.uint8)\n",
    "IMG.fromarray(255-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'sk_threshold': 150,\n",
    "#           'alpha': 7.0,\n",
    "#           'orig_contrib': 0.3,\n",
    "#           'seg_threshold': 24}\n",
    "\n",
    "img_obj = SegmentedImage()\n",
    "\n",
    "img_obj.load_file(data_dir=Dirs['images'], file_name='b21_training.tif')\n",
    "img_obj.res['orig'] = img_obj.image_arr[:, :, 1]\n",
    "img_obj.working_arr = img_obj.image_arr[:, :, 1]\n",
    "\n",
    "img_obj.load_mask(mask_dir=Dirs['mask'], fget_mask=get_mask_file, erode=True)\n",
    "img_obj.load_ground_truth(gt_dir=Dirs['truth'], fget_ground_truth=get_ground_truth_file)\n",
    "\n",
    "# img_obj.generate_skeleton(threshold=params['sk_threshold'])\n",
    "# img_obj.generate_lattice_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
