{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/Spring2018/ature_env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/home/akhanal1/Spring2018/pl-env/bin/python3.5\n",
    "# Torch imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ak/Spring2018/ature')\n",
    "os.chdir('/home/ak/Spring2018/ature')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from commons.segmentation import AtureTest\n",
    "from commons.IMAGE import SegmentedImage\n",
    "\n",
    "\n",
    "from utils import img_utils as imgutil\n",
    "from commons.IMAGE import Image\n",
    "from neuralnet.unet.unet_trainer import UnetNNTrainer\n",
    "from neuralnet.unet.unet_dataloader import ImageGenerator\n",
    "from neuralnet.unet.model.unet import UNet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define folders. Create if needed.\n",
    "sep = os.sep\n",
    "Dirs = {}\n",
    "Dirs['checkpoint']   = 'assests' +sep+ 'nnet_models'\n",
    "Dirs['data']      = 'data'+sep+'DRIVE'+sep+'training'\n",
    "Dirs['images']    = Dirs['data'] +sep+ 'images'\n",
    "Dirs['mask']      = Dirs['data'] +sep+ 'mask'\n",
    "Dirs['truth']     = Dirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "TestDirs = {}\n",
    "TestDirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "TestDirs['images']    = TestDirs['data'] +sep+ 'test_images'\n",
    "TestDirs['mask']      = TestDirs['data'] +sep+ 'mask'\n",
    "TestDirs['truth']     = TestDirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "ValidationDirs = {}\n",
    "ValidationDirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "ValidationDirs['images']    = ValidationDirs['data'] +sep+ 'validation_images'\n",
    "ValidationDirs['mask']      = ValidationDirs['data'] +sep+ 'mask'\n",
    "ValidationDirs['truth']     = ValidationDirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "for k, folder in Dirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "for k, folder in TestDirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "for k, folder in ValidationDirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def get_mask_file(file_name): \n",
    "    return file_name.split('_')[0] + '_training_mask.gif'\n",
    "\n",
    "def get_ground_truth_file(file_name): \n",
    "    return file_name.split('_')[0] + '_manual1.gif'\n",
    "\n",
    "def get_mask_file_test(file_name): \n",
    "    return file_name.split('_')[0] + '_test_mask.gif'\n",
    "\n",
    "train_image_size = (380, 380)\n",
    "classes = { 'background': 0, 'vessel': 1,}\n",
    "batch_size = 1\n",
    "num_classes = len(classes)\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "use_gpu = False\n",
    "\n",
    "#### Images to train/validate per epoch ####\n",
    "train_size = 50000\n",
    "validation_size = 5000\n",
    "checkpoint_file = 'PytorchCheckpoint51.nn.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(num_channels, num_classes)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 20 images found.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomRotation(5),\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "trainset = ImageGenerator(Dirs=Dirs,\n",
    "                          transform=transform,\n",
    "                          fget_mask=get_mask_file,\n",
    "                          fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 3 images found.\n"
     ]
    }
   ],
   "source": [
    "transform_val = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "validation_set = ImageGenerator(Dirs=ValidationDirs,\n",
    "                                transform=transform_val,\n",
    "                                fget_mask=get_mask_file_test,\n",
    "                                fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "validationloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, \n",
    "                                            shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epochs:[1/1] Batches:[1/20]   Loss: 0.613 accuracy: 0.726\n",
      "Epochs:[1/1] Batches:[2/20]   Loss: 0.548 accuracy: 0.815\n",
      "Epochs:[1/1] Batches:[3/20]   Loss: 0.531 accuracy: 0.804\n",
      "Epochs:[1/1] Batches:[4/20]   Loss: 0.539 accuracy: 0.763\n",
      "Epochs:[1/1] Batches:[5/20]   Loss: 0.542 accuracy: 0.794\n",
      "Epochs:[1/1] Batches:[6/20]   Loss: 0.492 accuracy: 0.827\n",
      "Epochs:[1/1] Batches:[7/20]   Loss: 0.480 accuracy: 0.841\n",
      "Epochs:[1/1] Batches:[8/20]   Loss: 0.461 accuracy: 0.848\n",
      "Epochs:[1/1] Batches:[9/20]   Loss: 0.430 accuracy: 0.868\n",
      "Epochs:[1/1] Batches:[10/20]   Loss: 0.446 accuracy: 0.834\n",
      "Epochs:[1/1] Batches:[11/20]   Loss: 0.448 accuracy: 0.844\n",
      "Epochs:[1/1] Batches:[12/20]   Loss: 0.427 accuracy: 0.859\n",
      "Epochs:[1/1] Batches:[13/20]   Loss: 0.448 accuracy: 0.825\n",
      "Epochs:[1/1] Batches:[14/20]   Loss: 0.403 accuracy: 0.858\n",
      "Epochs:[1/1] Batches:[15/20]   Loss: 0.430 accuracy: 0.844\n",
      "Epochs:[1/1] Batches:[16/20]   Loss: 0.407 accuracy: 0.858\n",
      "Epochs:[1/1] Batches:[17/20]   Loss: 0.392 accuracy: 0.876\n",
      "Epochs:[1/1] Batches:[18/20]   Loss: 0.432 accuracy: 0.838\n",
      "Epochs:[1/1] Batches:[19/20]   Loss: 0.373 accuracy: 0.878\n",
      "Epochs:[1/1] Batches:[20/20]   Loss: 0.409 accuracy: 0.851\n",
      "\n",
      "Evaluating...\n",
      "_________ACCURACY___of___[3/3]batches: 12247166.67%\n",
      "Accuracy improved from  0.0 to 12247166.67. Saving model..\n"
     ]
    }
   ],
   "source": [
    "trainer = UnetNNTrainer(model=net, checkpoint_dir=Dirs['checkpoint'], checkpoint_file=checkpoint_file)\n",
    "trainer.resume_from_checkpoint()\n",
    "# trainer.train(optimizer=optimizer, dataloader=trainloader, epochs=epochs, use_gpu=use_gpu, \n",
    "#               validationloader=validationloader, log_frequency=1)\n",
    "# trainer.resume_from_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1 images found.\n"
     ]
    }
   ],
   "source": [
    "transform_test = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "testset = ImageGenerator(Dirs=TestDirs,\n",
    "                         transform=transform_test,\n",
    "                         fget_mask=get_mask_file_test,\n",
    "                         fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, \n",
    "                                          shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "_________ACCURACY___of___[1/1]batches: 12586600.00%\n"
     ]
    }
   ],
   "source": [
    "y_scores, y_pred, y_true = trainer.evaluate(dataloader=testloader, use_gpu=use_gpu, force_checkpoint=False)\n",
    "# mnt.plot_confusion_matrix(y_pred=y_pred, y_true=y_true, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = y_pred.squeeze() * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAF8CAAAAADpjF8WAAAA4klEQVR4nO3YQQqAMAwEwMX//zleCooXb42YmReELWxKEgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABeVPcAwDSVHN0zjLT6XvjALhWd3034DKR2Gqy+T4S/T5V7zid4BID/qlwL99b3fjsbVMqOBZjMbaeX8AEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB4OAEl9xT+9T9C/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=380x380 at 0x7F7B91C98748>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG.fromarray(ig.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve throughout the image to generate segmented image based on trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = np.exp(scores.copy())\n",
    "# seg = np.zeros(testset.images[0].working_arr.shape)\n",
    "# for val in zip(IDs, IJs, sc):\n",
    "#     image_id, (i, j), (b_prob, v_prob) = val\n",
    "#     seg[i, j] = 255 * v_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG.fromarray(seg.astype(np.uint8)).save(checkpoint_file+testset.images[0].file_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'sk_threshold': 150,\n",
    "          'alpha': 7.0,\n",
    "          'orig_contrib': 0.3,\n",
    "          'seg_threshold': 24}\n",
    "\n",
    "img_obj = SegmentedImage()\n",
    "\n",
    "img_obj.load_file(data_dir=TestDirs['images'], file_name='01_test.tif')\n",
    "img_obj.res['orig'] = img_obj.image_arr[:, :, 1]\n",
    "# img_obj.working_arr = 255 - seg.astype(np.uint8)\n",
    "\n",
    "# img_obj.load_mask(mask_dir=TestDirs['mask'], fget_mask=get_mask_file_test, erode=True)\n",
    "# img_obj.load_ground_truth(gt_dir=TestDirs['truth'], fget_ground_truth=get_ground_truth_file)\n",
    "\n",
    "# img_obj.generate_skeleton(threshold=params['sk_threshold'])\n",
    "# img_obj.generate_lattice_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester = AtureTest(out_dir='')\n",
    "# tester.run(params=params, save_images=False, img_obj=img_obj)\n",
    "# img_obj.res['scores']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
