{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda/envs/ature_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/home/akhanal1/Spring2018/pl-env/bin/python3.5\n",
    "# Torch imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ak/Spring2018/ature')\n",
    "os.chdir('/home/ak/Spring2018/ature')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from commons.segmentation import AtureTest\n",
    "from commons.IMAGE import SegmentedImage\n",
    "\n",
    "\n",
    "from utils import img_utils as imgutil\n",
    "from commons.IMAGE import Image\n",
    "from neuralnet.unet.unet_trainer import UnetNNTrainer\n",
    "from neuralnet.unet.unet_dataloader import ImageGenerator\n",
    "from neuralnet.unet.model.unet import UNet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define folders. Create if needed.\n",
    "sep = os.sep\n",
    "Dirs = {}\n",
    "Dirs['checkpoint']   = 'assests' +sep+ 'nnet_models'\n",
    "Dirs['data']      = 'data'+sep+'DRIVE'+sep+'training'\n",
    "Dirs['images']    = Dirs['data'] +sep+ 'images'\n",
    "Dirs['mask']      = Dirs['data'] +sep+ 'mask'\n",
    "Dirs['truth']     = Dirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "TestDirs = {}\n",
    "TestDirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "TestDirs['images']    = TestDirs['data'] +sep+ 'test_images'\n",
    "TestDirs['mask']      = TestDirs['data'] +sep+ 'mask'\n",
    "TestDirs['truth']     = TestDirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "ValidationDirs = {}\n",
    "ValidationDirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "ValidationDirs['images']    = ValidationDirs['data'] +sep+ 'validation_images'\n",
    "ValidationDirs['mask']      = ValidationDirs['data'] +sep+ 'mask'\n",
    "ValidationDirs['truth']     = ValidationDirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "for k, folder in Dirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "for k, folder in TestDirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "for k, folder in ValidationDirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def get_mask_file(file_name): \n",
    "    return file_name.split('_')[0] + '_training_mask.gif'\n",
    "\n",
    "def get_ground_truth_file(file_name): \n",
    "    return file_name.split('_')[0] + '_manual1.gif'\n",
    "\n",
    "def get_mask_file_test(file_name): \n",
    "    return file_name.split('_')[0] + '_test_mask.gif'\n",
    "\n",
    "train_image_size = (380, 380)\n",
    "classes = { 'background': 0, 'vessel': 1,}\n",
    "batch_size = 2\n",
    "num_classes = len(classes)\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "use_gpu = False\n",
    "\n",
    "#### Images to train/validate per epoch ####\n",
    "train_size = 50000\n",
    "validation_size = 5000\n",
    "checkpoint_file = 'PytorchCheckpoint51.nn.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(num_channels, num_classes)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 20 images found.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomRotation(5),\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "trainset = ImageGenerator(Dirs=Dirs,\n",
    "                          transform=transform,\n",
    "                          fget_mask=get_mask_file,\n",
    "                          fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 3 images found.\n"
     ]
    }
   ],
   "source": [
    "transform_val = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "validation_set = ImageGenerator(Dirs=ValidationDirs,\n",
    "                                transform=transform_val,\n",
    "                                fget_mask=get_mask_file_test,\n",
    "                                fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "validationloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, \n",
    "                                            shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epochs:[1/5] Batches:[1/10]  LOSS:1.083 precision:0.138 recall:0.817 f1:0.237 supp:0.000\n",
      "Epochs:[1/5] Batches:[2/10]  LOSS:1.022 precision:0.161 recall:0.871 f1:0.272 supp:0.000\n",
      "Epochs:[1/5] Batches:[3/10]  LOSS:0.966 precision:0.164 recall:0.855 f1:0.275 supp:0.000\n",
      "Epochs:[1/5] Batches:[4/10]  LOSS:1.006 precision:0.119 recall:0.860 f1:0.210 supp:0.000\n",
      "Epochs:[1/5] Batches:[5/10]  LOSS:0.914 precision:0.154 recall:0.867 f1:0.261 supp:0.000\n",
      "Epochs:[1/5] Batches:[6/10]  LOSS:0.897 precision:0.144 recall:0.928 f1:0.250 supp:0.000\n",
      "Epochs:[1/5] Batches:[7/10]  LOSS:0.806 precision:0.189 recall:0.790 f1:0.304 supp:0.000\n",
      "Epochs:[1/5] Batches:[8/10]  LOSS:0.751 precision:0.215 recall:0.767 f1:0.336 supp:0.000\n",
      "Epochs:[1/5] Batches:[9/10]  LOSS:0.880 precision:0.155 recall:0.839 f1:0.261 supp:0.000\n",
      "Epochs:[1/5] Batches:[10/10]  LOSS:0.749 precision:0.208 recall:0.746 f1:0.325 supp:0.000\n",
      "\n",
      "Evaluating...\n",
      "Batch[2/2] Precision:0.156 Recall:0.089 F1:0.113 Supp:0.000\n",
      "Final  #Precision:0.209 #Recall:0.063 #F1:0.097 #Supp:0.000\n",
      "Score improved from  0.0 to 0.097. Saving model..\n",
      "Epochs:[2/5] Batches:[1/10]  LOSS:0.618 precision:0.210 recall:0.029 f1:0.051 supp:0.000\n",
      "Epochs:[2/5] Batches:[2/10]  LOSS:0.605 precision:0.212 recall:0.117 f1:0.151 supp:0.000\n",
      "Epochs:[2/5] Batches:[3/10]  LOSS:0.526 precision:0.203 recall:0.012 f1:0.023 supp:0.000\n",
      "Epochs:[2/5] Batches:[4/10]  LOSS:0.487 precision:0.042 recall:0.001 f1:0.001 supp:0.000\n",
      "Epochs:[2/5] Batches:[5/10]  LOSS:0.436 precision:0.134 recall:0.000 f1:0.001 supp:0.000\n",
      "Epochs:[2/5] Batches:[6/10]  LOSS:0.411 precision:0.125 recall:0.001 f1:0.001 supp:0.000\n",
      "Epochs:[2/5] Batches:[7/10]  LOSS:0.408 precision:1.000 recall:0.000 f1:0.000 supp:0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda/envs/ature_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:[2/5] Batches:[8/10]  LOSS:0.418 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[2/5] Batches:[9/10]  LOSS:0.360 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[2/5] Batches:[10/10]  LOSS:0.388 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "\n",
      "Evaluating...\n",
      "Batch[2/2] Precision:0.000 Recall:0.000 F1:0.000 Supp:0.000\n",
      "Final  #Precision:0.000 #Recall:0.000 #F1:0.000 #Supp:0.000\n",
      "Score did not improve. _was:0.097\n",
      "Epochs:[3/5] Batches:[1/10]  LOSS:0.426 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[2/10]  LOSS:0.499 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[3/10]  LOSS:0.470 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[4/10]  LOSS:0.350 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[5/10]  LOSS:0.385 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[6/10]  LOSS:0.351 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[7/10]  LOSS:0.395 precision:1.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[8/10]  LOSS:0.409 precision:0.400 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[9/10]  LOSS:0.365 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[3/5] Batches:[10/10]  LOSS:0.376 precision:0.626 recall:0.002 f1:0.003 supp:0.000\n",
      "\n",
      "Evaluating...\n",
      "Batch[2/2] Precision:0.000 Recall:0.000 F1:0.000 Supp:0.000\n",
      "Final  #Precision:0.187 #Recall:0.001 #F1:0.001 #Supp:0.000\n",
      "Score did not improve. _was:0.097\n",
      "Epochs:[4/5] Batches:[1/10]  LOSS:0.394 precision:0.640 recall:0.000 f1:0.001 supp:0.000\n",
      "Epochs:[4/5] Batches:[2/10]  LOSS:0.432 precision:0.281 recall:0.002 f1:0.005 supp:0.000\n",
      "Epochs:[4/5] Batches:[3/10]  LOSS:0.423 precision:0.812 recall:0.000 f1:0.001 supp:0.000\n",
      "Epochs:[4/5] Batches:[4/10]  LOSS:0.348 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[4/5] Batches:[5/10]  LOSS:0.375 precision:0.553 recall:0.002 f1:0.003 supp:0.000\n",
      "Epochs:[4/5] Batches:[6/10]  LOSS:0.369 precision:0.195 recall:0.033 f1:0.057 supp:0.000\n",
      "Epochs:[4/5] Batches:[7/10]  LOSS:0.394 precision:0.905 recall:0.001 f1:0.002 supp:0.000\n",
      "Epochs:[4/5] Batches:[8/10]  LOSS:0.409 precision:0.444 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[4/5] Batches:[9/10]  LOSS:0.353 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[4/5] Batches:[10/10]  LOSS:0.370 precision:0.500 recall:0.000 f1:0.000 supp:0.000\n",
      "\n",
      "Evaluating...\n",
      "Batch[2/2] Precision:0.000 Recall:0.000 F1:0.000 Supp:0.000\n",
      "Final  #Precision:0.000 #Recall:0.000 #F1:0.000 #Supp:0.000\n",
      "Score did not improve. _was:0.097\n",
      "Epochs:[5/5] Batches:[1/10]  LOSS:0.396 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[5/5] Batches:[2/10]  LOSS:0.447 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[5/5] Batches:[3/10]  LOSS:0.431 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[5/5] Batches:[4/10]  LOSS:0.345 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[5/5] Batches:[5/10]  LOSS:0.374 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[5/5] Batches:[6/10]  LOSS:0.345 precision:0.133 recall:0.000 f1:0.001 supp:0.000\n",
      "Epochs:[5/5] Batches:[7/10]  LOSS:0.392 precision:0.808 recall:0.001 f1:0.001 supp:0.000\n",
      "Epochs:[5/5] Batches:[8/10]  LOSS:0.402 precision:0.500 recall:0.000 f1:0.001 supp:0.000\n",
      "Epochs:[5/5] Batches:[9/10]  LOSS:0.354 precision:0.000 recall:0.000 f1:0.000 supp:0.000\n",
      "Epochs:[5/5] Batches:[10/10]  LOSS:0.370 precision:0.600 recall:0.000 f1:0.001 supp:0.000\n",
      "\n",
      "Evaluating...\n",
      "Batch[2/2] Precision:0.333 Recall:0.000 F1:0.000 Supp:0.000\n",
      "Final  #Precision:0.542 #Recall:0.000 #F1:0.000 #Supp:0.000\n",
      "Score did not improve. _was:0.097\n"
     ]
    }
   ],
   "source": [
    "trainer = UnetNNTrainer(model=net, checkpoint_dir=Dirs['checkpoint'], checkpoint_file=checkpoint_file)\n",
    "# trainer.resume_from_checkpoint()\n",
    "trainer.train(optimizer=optimizer, dataloader=trainloader, epochs=epochs, use_gpu=use_gpu, \n",
    "              validationloader=validationloader, log_frequency=1)\n",
    "# trainer.resume_from_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1 images found.\n"
     ]
    }
   ],
   "source": [
    "transform_test = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "testset = ImageGenerator(Dirs=TestDirs,\n",
    "                         transform=transform_test,\n",
    "                         fget_mask=get_mask_file_test,\n",
    "                         fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, \n",
    "                                          shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "Batch[1/1] Precision:0.000 Recall:0.000 F1:0.000 Supp:0.000\n",
      "Final  #Precision:0.000 #Recall:0.000 #F1:0.000 #Supp:0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda/envs/ature_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_scores, y_pred, y_true = trainer.evaluate(dataloader=testloader, use_gpu=use_gpu, force_checkpoint=False)\n",
    "# mnt.plot_confusion_matrix(y_pred=y_pred, y_true=y_true, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ig = y_pred.squeeze() * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAF8CAAAAADpjF8WAAAAo0lEQVR4nO3BgQAAAADDoPlTX+EAVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8Bo1qgABTkpWoQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=380x380 at 0x7FC570093A58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG.fromarray(ig.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG.fromarray(ig.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve throughout the image to generate segmented image based on trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc = np.exp(scores.copy())\n",
    "# seg = np.zeros(testset.images[0].working_arr.shape)\n",
    "# for val in zip(IDs, IJs, sc):\n",
    "#     image_id, (i, j), (b_prob, v_prob) = val\n",
    "#     seg[i, j] = 255 * v_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG.fromarray(seg.astype(np.uint8)).save(checkpoint_file+testset.images[0].file_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'sk_threshold': 150,\n",
    "          'alpha': 7.0,\n",
    "          'orig_contrib': 0.3,\n",
    "          'seg_threshold': 24}\n",
    "\n",
    "img_obj = SegmentedImage()\n",
    "\n",
    "img_obj.load_file(data_dir=TestDirs['images'], file_name='01_test.tif')\n",
    "img_obj.res['orig'] = img_obj.image_arr[:, :, 1]\n",
    "# img_obj.working_arr = 255 - seg.astype(np.uint8)\n",
    "\n",
    "# img_obj.load_mask(mask_dir=TestDirs['mask'], fget_mask=get_mask_file_test, erode=True)\n",
    "# img_obj.load_ground_truth(gt_dir=TestDirs['truth'], fget_ground_truth=get_ground_truth_file)\n",
    "\n",
    "# img_obj.generate_skeleton(threshold=params['sk_threshold'])\n",
    "# img_obj.generate_lattice_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tester = AtureTest(out_dir='')\n",
    "# tester.run(params=params, save_images=False, img_obj=img_obj)\n",
    "# img_obj.res['scores']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
