{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda/envs/ature_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/home/akhanal1/Spring2018/pl-env/bin/python3.5\n",
    "# Torch imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ak/Spring2018/ature')\n",
    "os.chdir('/home/ak/Spring2018/ature')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from commons.segmentation import AtureTest\n",
    "from commons.IMAGE import SegmentedImage\n",
    "\n",
    "\n",
    "from utils import img_utils as imgutil\n",
    "from commons.IMAGE import Image\n",
    "from neuralnet.unet.unet_trainer import UnetNNTrainer\n",
    "from neuralnet.unet.unet_dataloader import ImageGenerator\n",
    "from neuralnet.unet.model.unet import UNet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define folders. Create if needed.\n",
    "sep = os.sep\n",
    "Dirs = {}\n",
    "Dirs['checkpoint']   = 'assests' +sep+ 'nnet_models'\n",
    "Dirs['data']      = 'data'+sep+'DRIVE'+sep+'training'\n",
    "Dirs['images']    = Dirs['data'] +sep+ 'images'\n",
    "Dirs['mask']      = Dirs['data'] +sep+ 'mask'\n",
    "Dirs['truth']     = Dirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "TestDirs = {}\n",
    "TestDirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "TestDirs['images']    = TestDirs['data'] +sep+ 'test_images'\n",
    "TestDirs['mask']      = TestDirs['data'] +sep+ 'mask'\n",
    "TestDirs['truth']     = TestDirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "ValidationDirs = {}\n",
    "ValidationDirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "ValidationDirs['images']    = ValidationDirs['data'] +sep+ 'validation_images'\n",
    "ValidationDirs['mask']      = ValidationDirs['data'] +sep+ 'mask'\n",
    "ValidationDirs['truth']     = ValidationDirs['data'] +sep+ '1st_manual'\n",
    "\n",
    "for k, folder in Dirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "for k, folder in TestDirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "for k, folder in ValidationDirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def get_mask_file(file_name): \n",
    "    return file_name.split('_')[0] + '_training_mask.gif'\n",
    "\n",
    "def get_ground_truth_file(file_name): \n",
    "    return file_name.split('_')[0] + '_manual1.gif'\n",
    "\n",
    "def get_mask_file_test(file_name): \n",
    "    return file_name.split('_')[0] + '_test_mask.gif'\n",
    "\n",
    "train_image_size = (380, 380)\n",
    "classes = { 'background': 0, 'vessel': 1,}\n",
    "batch_size = 2\n",
    "num_classes = len(classes)\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "use_gpu = False\n",
    "\n",
    "#### Images to train/validate per epoch ####\n",
    "train_size = 50000\n",
    "validation_size = 5000\n",
    "checkpoint_file = 'PytorchCheckpoint51.nn.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = UNet(num_channels, num_classes)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 20 images found.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomRotation(5),\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "trainset = ImageGenerator(Dirs=Dirs,\n",
    "                          transform=transform,\n",
    "                          fget_mask=get_mask_file,\n",
    "                          fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 3 images found.\n"
     ]
    }
   ],
   "source": [
    "transform_val = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "validation_set = ImageGenerator(Dirs=ValidationDirs,\n",
    "                                transform=transform_val,\n",
    "                                fget_mask=get_mask_file_test,\n",
    "                                fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "validationloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, \n",
    "                                            shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epochs:[1/3] Batches:[1/10], loss:0.810, pre:0.159 rec:0.681 f1:0.258 acc:0.474\n",
      "Epochs:[1/3] Batches:[2/10], loss:0.772, pre:0.196 rec:0.632 f1:0.299 acc:0.523\n",
      "Epochs:[1/3] Batches:[3/10], loss:0.698, pre:0.241 rec:0.649 f1:0.352 acc:0.635\n",
      "Epochs:[1/3] Batches:[4/10], loss:0.705, pre:0.160 rec:0.601 f1:0.252 acc:0.606\n",
      "Epochs:[1/3] Batches:[5/10], loss:0.656, pre:0.258 rec:0.668 f1:0.372 acc:0.711\n",
      "Epochs:[1/3] Batches:[6/10], loss:0.646, pre:0.231 rec:0.620 f1:0.336 acc:0.721\n",
      "Epochs:[1/3] Batches:[7/10], loss:0.621, pre:0.265 rec:0.552 f1:0.358 acc:0.733\n",
      "Epochs:[1/3] Batches:[8/10], loss:0.598, pre:0.299 rec:0.557 f1:0.389 acc:0.751\n",
      "Epochs:[1/3] Batches:[9/10], loss:0.625, pre:0.254 rec:0.630 f1:0.362 acc:0.746\n",
      "Epochs:[1/3] Batches:[10/10], loss:0.573, pre:0.279 rec:0.548 f1:0.370 acc:0.771\n",
      "\n",
      "Evaluating...\n",
      "Batch[2/2] pre:0.232 rec:0.014 f1:0.026 acc:0.853\n",
      "FINAL::: #Precision:0.196 #Recall:0.014 #F1:0.025 #Acc:0.842\n",
      "Score improved from  0.0 to 0.025. Saving model..\n",
      "Epochs:[2/3] Batches:[1/10], loss:0.465, pre:0.171 rec:0.015 f1:0.028 acc:0.858\n",
      "Epochs:[2/3] Batches:[2/10], loss:0.491, pre:0.188 rec:0.048 f1:0.077 acc:0.813\n",
      "Epochs:[2/3] Batches:[3/10], loss:0.446, pre:0.205 rec:0.003 f1:0.006 acc:0.846\n",
      "Epochs:[2/3] Batches:[4/10], loss:0.374, pre:0.000 rec:0.000 f1:0.000 acc:0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda/envs/ature_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:[2/3] Batches:[5/10], loss:0.386, pre:0.000 rec:0.000 f1:0.000 acc:0.872\n",
      "Epochs:[2/3] Batches:[6/10], loss:0.357, pre:0.000 rec:0.000 f1:0.000 acc:0.886\n",
      "Epochs:[2/3] Batches:[7/10], loss:0.398, pre:0.000 rec:0.000 f1:0.000 acc:0.865\n",
      "Epochs:[2/3] Batches:[8/10], loss:0.417, pre:0.000 rec:0.000 f1:0.000 acc:0.858\n",
      "Epochs:[2/3] Batches:[9/10], loss:0.361, pre:0.000 rec:0.000 f1:0.000 acc:0.885\n",
      "Epochs:[2/3] Batches:[10/10], loss:0.385, pre:0.000 rec:0.000 f1:0.000 acc:0.877\n",
      "\n",
      "Evaluating...\n",
      "Batch[2/2] pre:0.000 rec:0.000 f1:0.000 acc:0.858\n",
      "FINAL::: #Precision:0.000 #Recall:0.000 #F1:0.000 #Acc:0.848\n",
      "Score did not improve. _was:0.025\n",
      "Epochs:[3/3] Batches:[1/10], loss:0.410, pre:0.000 rec:0.000 f1:0.000 acc:0.866\n",
      "Epochs:[3/3] Batches:[2/10], loss:0.476, pre:0.000 rec:0.000 f1:0.000 acc:0.839\n",
      "Epochs:[3/3] Batches:[3/10], loss:0.446, pre:0.000 rec:0.000 f1:0.000 acc:0.847\n",
      "Epochs:[3/3] Batches:[4/10], loss:0.351, pre:0.000 rec:0.000 f1:0.000 acc:0.889\n",
      "Epochs:[3/3] Batches:[5/10], loss:0.382, pre:0.000 rec:0.000 f1:0.000 acc:0.872\n",
      "Epochs:[3/3] Batches:[6/10], loss:0.354, pre:0.000 rec:0.000 f1:0.000 acc:0.886\n",
      "Epochs:[3/3] Batches:[7/10], loss:0.395, pre:0.000 rec:0.000 f1:0.000 acc:0.865\n",
      "Epochs:[3/3] Batches:[8/10], loss:0.408, pre:0.000 rec:0.000 f1:0.000 acc:0.858\n",
      "Epochs:[3/3] Batches:[9/10], loss:0.359, pre:0.000 rec:0.000 f1:0.000 acc:0.885\n",
      "Epochs:[3/3] Batches:[10/10], loss:0.371, pre:0.000 rec:0.000 f1:0.000 acc:0.877\n",
      "\n",
      "Evaluating...\n",
      "Batch[2/2] pre:0.000 rec:0.000 f1:0.000 acc:0.858\n",
      "FINAL::: #Precision:0.000 #Recall:0.000 #F1:0.000 #Acc:0.848\n",
      "Score did not improve. _was:0.025\n"
     ]
    }
   ],
   "source": [
    "trainer = UnetNNTrainer(model=net, checkpoint_dir=Dirs['checkpoint'], checkpoint_file=checkpoint_file)\n",
    "# trainer.resume_from_checkpoint()\n",
    "trainer.train(optimizer=optimizer, dataloader=trainloader, epochs=epochs, use_gpu=use_gpu, \n",
    "              validationloader=validationloader, log_frequency=1)\n",
    "# trainer.resume_from_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1 images found.\n"
     ]
    }
   ],
   "source": [
    "transform_test = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "testset = ImageGenerator(Dirs=TestDirs,\n",
    "                         transform=transform_test,\n",
    "                         fget_mask=get_mask_file_test,\n",
    "                         fget_truth=get_ground_truth_file, train_image_size=train_image_size) \n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, \n",
    "                                          shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "Batch[1/1] pre:0.000 rec:0.000 f1:0.000 acc:0.872\n",
      "FINAL::: #Precision:0.000 #Recall:0.000 #F1:0.000 #Acc:0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/anaconda/envs/ature_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_scores, y_pred, y_true = trainer.evaluate(dataloader=testloader, use_gpu=use_gpu, force_checkpoint=False)\n",
    "# mnt.plot_confusion_matrix(y_pred=y_pred, y_true=y_true, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ig = y_pred.squeeze() * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAF8CAAAAADpjF8WAAAAo0lEQVR4nO3BgQAAAADDoPlTX+EAVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8Bo1qgABTkpWoQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=380x380 at 0x7FB7645AEB38>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG.fromarray(ig.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG.fromarray(ig.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve throughout the image to generate segmented image based on trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc = np.exp(scores.copy())\n",
    "# seg = np.zeros(testset.images[0].working_arr.shape)\n",
    "# for val in zip(IDs, IJs, sc):\n",
    "#     image_id, (i, j), (b_prob, v_prob) = val\n",
    "#     seg[i, j] = 255 * v_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG.fromarray(seg.astype(np.uint8)).save(checkpoint_file+testset.images[0].file_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'sk_threshold': 150,\n",
    "          'alpha': 7.0,\n",
    "          'orig_contrib': 0.3,\n",
    "          'seg_threshold': 24}\n",
    "\n",
    "img_obj = SegmentedImage()\n",
    "\n",
    "img_obj.load_file(data_dir=TestDirs['images'], file_name='01_test.tif')\n",
    "img_obj.res['orig'] = img_obj.image_arr[:, :, 1]\n",
    "# img_obj.working_arr = 255 - seg.astype(np.uint8)\n",
    "\n",
    "# img_obj.load_mask(mask_dir=TestDirs['mask'], fget_mask=get_mask_file_test, erode=True)\n",
    "# img_obj.load_ground_truth(gt_dir=TestDirs['truth'], fget_ground_truth=get_ground_truth_file)\n",
    "\n",
    "# img_obj.generate_skeleton(threshold=params['sk_threshold'])\n",
    "# img_obj.generate_lattice_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tester = AtureTest(out_dir='')\n",
    "# tester.run(params=params, save_images=False, img_obj=img_obj)\n",
    "# img_obj.res['scores']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
