{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook loads and train data from the directories without having them to load on memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Projects\\\\ature')\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "\n",
    "import neuralnet.utils.data_utils as dutils\n",
    "from commons.IMAGE import SegmentedImage\n",
    "from PIL import Image as IMG\n",
    "import utils.img_utils as imgutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = os.sep\n",
    "Dirs = {}\n",
    "\n",
    "Dirs['train_data']= 'data'+sep+'DRIVE'+sep+'training'+sep +'patches'\n",
    "Dirs['test_data'] = 'data'+sep+'DRIVE'+sep+'test' +sep+ 'patches'\n",
    "Dirs['checkpoint']   = 'data' +sep+ 'checkpoint'\n",
    "\n",
    "Dirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "Dirs['images']    = Dirs['data'] +sep+ 'images'\n",
    "Dirs['mask']      = Dirs['data'] +sep+ 'mask'\n",
    "Dirs['truth']     = Dirs['data'] +sep+ '1st_manual'\n",
    "Dirs['segmented'] = Dirs['data'] +sep+ 'drive_segmented'\n",
    "\n",
    "for k, folder in Dirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "batch_size = 32\n",
    "num_classes = 4\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62810 images belonging to 4 classes.\n",
      "Found 9802 images belonging to 4 classes.\n",
      "Found 9802 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        Dirs['train_data'],\n",
    "        target_size=(31, 31),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        Dirs['test_data'],\n",
    "        target_size=(31, 31),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical', \n",
    "        seed=10)\n",
    "\n",
    "test_generator = validation_datagen.flow_from_directory(\n",
    "        Dirs['test_data'],\n",
    "        target_size=(31, 31),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None, \n",
    "        seed=11,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and test data: CAREFUL!!! Memory Intensive task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = dutils.load_dataset(data_path=Dirs['train_data'], img_shape=(31,31,1), num_classes=num_classes)\n",
    "# x_test, y_test = dutils.load_dataset(data_path=Dirs['test_data'], img_shape=(31,31,1), num_classes=num_classes)\n",
    "\n",
    "# class_weights = dutils.get_class_weights(y_train)\n",
    "\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# train_datagen.fit(x_train)\n",
    "# test_datagen.fit(x_test)\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(31,31,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model from x_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model from directory generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 415s 207ms/step - loss: 0.2886 - acc: 0.9153 - val_loss: 0.3198 - val_acc: 0.9024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16844f0d208>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci = train_generator.class_indices\n",
    "class_weights = {ci['green']: 15, ci['red']: 7, ci['white']: 5. ci['black']: 1 }\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=10000,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800, verbose=1, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If we shuffle, we wont be able to generate a segmented image.\n",
    "test_generator.shuffle = False\n",
    "prediction = model.predict_generator(test_generator)\n",
    "pred = np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_obj = SegmentedImage()\n",
    "img_obj.load_file(data_dir=Dirs['images'], file_name='01_test.tif')\n",
    "img_obj.working_arr = img_obj.image_arr[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = np.zeros_like(img_obj.working_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 3], dtype=int64), array([7524, 2278], dtype=int64))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, f in enumerate(test_generator.filenames):\n",
    "    i,j = f.split('\\\\')[1].split(' ')[1].split('.')[0].split('_')\n",
    "    if pred[ix]==ci['white'] or pred[ix]==ci['red']:\n",
    "        seg[int(i),int(j)] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAJICAAAAAC8wkpbAAAElklEQVR4nO3c0ZqbKhQG0M35+v6v7LmYxBoERNs0GNa6mjGJQf1ni6ATAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACctCyfbsGQ/vt0A25FiH5IzYkgLBGL4ETEr0834NPOhSYi0ubXVH3vd1NrTkqVn6ciNU3bXCwRkdLm12lDIzUHtil5CdHMoZm+X9PdrclDk/YvR8xy0po9NVeP88unlu2PM+RGavpkpSZ7ZTVDZEJqOi0R61lpiVjjMWNkYqYtLeo8oex7PylbOtd+VGs6POKRIpa0pCUi0qRF5mG6DX7VVWt+h+Z1QeTLp6HWHGlfU08YmZg9NcejNes7UmHZpJmZPTWXlII0GTMKfQqdmnlDM3mtSUenqN3rMhOh1rTtMiI0ETF7rTlQDc3cmZGaus3JKQvN7JmRmorS0K9Cs5p9F5QGh7MusNDsqDW58mWVzGxJTWbbA152S4UmIuyG/FbO1w7vz600LrdzdsRLbPaPPG107qtsHd/oqzeux5LPFuxD9NDcVbVB5u/cv9P3a7aTCvWbg1uHf8JHeL/zb+GUtdpsbwiO30tit7j0hrIv3b03qjX5I0h/y7Pa7EOzvqPyyf7MfNmo8m22453P4ldLSj1I3T3l8o3qN3eXWvORf+DQGZqTDfuCC/l7pOYfdTgrT+EW/JWbQFuhHNs9UvNv9Pdp+oeKD/Oe9Xhal3HjGL+F8e4/ytoZo/6t/eN4F4vk6EdFrTnfzTgdmueDmqkwDl353Ni5Gbt1P95Zauqd2mo2uptTOI/tRw9uOah8g1rzvq5w6zqo+q29zSnWsLSLTcrXuU6cDpybgZv28K7JwPZlUG1Yrnu4rnziO9HZHfmfRQ7bsKf3hKY52tL6XxGdzbk0brh/56hHZ9R2Pb0jNO2ncDOl22+OmtMoY73bM/ZtYGO2anU5NIf1ovNqJgrZOmjOwQhgV7UZfN5q1Hb9uBiaXQr2E9mVG68qXh9t6b3RpjUX0d6m0UeNB27a1dAUc5BNP6f6OytS31mja6rhqMsyemiGbtul0NSTcD4pB6tqfvdxQWpOk498XIZu3aVz+5+Gor+707+egqPZ9IEPS8TIzbtSaYqjZV0u3qt3bm27FVdGisY9KA/DNvBCaKod0e5rpMP19umebqjMVwx7UJ5GbeCFDmH76qV86Hu/4Vxw+tZa3MQ7VJphm/hHoSl/Lj/wg245AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA38j/gzdF0IZDOVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=565x584 at 0x168457A65F8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG.fromarray(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
