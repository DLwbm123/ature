{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('/home/ak/Spring2018/ature')\n",
    "\n",
    "from neuralnet.utils.datasets import DriveDatasetFromFile4Way, DriveDatasetFromImageObj4Way\n",
    "\n",
    "from utils import img_utils as imgutil\n",
    "\n",
    "sep = os.sep\n",
    "\n",
    "# Define folders (create them if needed)\n",
    "Dirs = {}\n",
    "Dirs['train_data']      = 'data'+sep+'DRIVE'+sep+'training'+sep +'patches'\n",
    "\n",
    "Dirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "Dirs['images']    = Dirs['data'] +sep+ 'images'\n",
    "Dirs['mask']      = Dirs['data'] +sep+ 'mask'\n",
    "Dirs['truth']     = Dirs['data'] +sep+ '1st_manual'\n",
    "Dirs['segmented'] = Dirs['data'] +sep+ 'drive_segmented'\n",
    "Dirs['test_data'] = Dirs['data'] +sep+ 'patches'\n",
    "\n",
    "def get_mask_file(file_name): \n",
    "    return file_name.split('_')[0] + '_test_mask.gif'\n",
    "\n",
    "def get_ground_truth_file(file_name): \n",
    "    return file_name.split('_')[0] + '_manual1.gif'\n",
    "\n",
    "input_image = '19_test.tif'\n",
    "\n",
    "# Set up execution flags\n",
    "Flags = {}\n",
    "Flags['useGPU'] = False\n",
    "\n",
    "classes = ('white', 'green', 'black', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file: data/DRIVE/training/patches/25_training.npy\n",
      "Data file: data/DRIVE/training/patches/24_training.npy\n",
      "Data file: data/DRIVE/training/patches/21_training.npy\n",
      "Data file: data/DRIVE/training/patches/22_training.npy\n",
      "Data file: data/DRIVE/training/patches/23_training.npy\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "trainset = DriveDatasetFromFile4Way(data_path=Dirs['train_data'], height=31, width=31, transform=transform)\n",
    "clss, class_counts = np.unique(trainset.labels, return_counts=True)\n",
    "class_weights = 1.0/class_counts\n",
    "data_weights = np.array([class_weights[t] for t in trainset.labels]) \n",
    "second_min_class_count =  np.partition(class_counts, 1)[1]\n",
    "\n",
    "sampler = WeightedRandomSampler(data_weights, int(second_min_class_count))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False, num_workers=1, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAAAAAA6xUnlAAABuklEQVR4nCXR0QEjWg1DQR3ZyVIh/XcBbK4lPl4LM/wbSeqLa52QW0NDtaP9cy+q5s97abF8klqdN8f+TQq63/jk3v4VuNQ8XLMG2G+l6/B29E7SybTbY8iP+TCR7BOtFNPCiqp8IJV63UNtGuyc9j/sojZJjJtRpcrWY7Tf1cUmPaPKFAytVunyGysRIyYnR9hIbVSWkeQCNJn+QHdqZVQ2+PWbByS8fCRUWZXY7HFePKEBIRL37Kodds8qEr6h4ACMdMW59fzGj/17zx9xD5paN6Py2ciJ8/Q/fCnsyaqcDnTjwwdvsaLhJ2H9I6gsM+kn91Hw9E5Wz4pR/NlB4p+LXsP2rsI8r3sbJTyhtKhXG43Ev+7Yrv67W/WuX14gYYb23kxfdz/vrOgj11R23Yu0SaU9C3lQfjeT8MzkRM4fdQnoWsq3v0pummCWitVc6WVGrdVKsis9wNo+qKf38zq33/aehObiZknRVXzvZqd/MShB04NNl7qizs/x3DVj/fyl6vrO2x/ak4SZ53n9kuYGvz+TF/FtC1Zu4Ou/Hbdi51hD/1IJcxY95uT1vR1xJ0kHLWrdCusF4P8/Kn33jkOp8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=31x31 at 0x7F32CFAF7B00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG.fromarray(trainset.data[3000].reshape(31,31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output width { conv: 16.0, maxpool: 8.0 }\n",
      "output width { conv: 6.0, maxpool: 6.0 }\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, width, channels):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.width = width\n",
    "        \n",
    "        \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 2      \n",
    "        self.kern_padding = 2\n",
    "        \n",
    "        self.mxp_kern_size = 2\n",
    "        self.mxp_stride = 2 \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.channels, 20, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 1      \n",
    "        self.kern_padding = 1\n",
    "        \n",
    "        self.mxp_kern_size = 1\n",
    "        self.mxp_stride = 1 \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(20, 50, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.linearWidth = 50*int(self.width)*int(self.width)\n",
    "        self.fc1 = nn.Linear(self.linearWidth, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "        self.fc3 = nn.Linear(10, 4)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.linearWidth)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def _update_output_size(self):       \n",
    "        self.width = ((self.width - self.kern_size + 2 * self.kern_padding) / self.kern_stride) + 1\n",
    "        temp = self.width\n",
    "        self.width = ((self.width - self.mxp_kern_size)/self.mxp_stride) + 1\n",
    "        print('output width { conv: ' + str(temp) + ', maxpool: ' + str(self.width) + ' }')\n",
    "\n",
    "width = 31\n",
    "channels = 1\n",
    "net = Net(width, channels)\n",
    "\n",
    "# Send network to the GPU, if requested\n",
    "if Flags['useGPU']:\n",
    "    net.cuda()\n",
    "\n",
    "# Define loss criterion\n",
    "# criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batches:  1000] loss: 1.385\n",
      "[epoch: 1, batches:  2000] loss: 1.381\n",
      "[epoch: 1, batches:  3000] loss: 1.349\n",
      "[epoch: 1, batches:  4000] loss: 1.181\n",
      "[epoch: 2, batches:  1000] loss: 1.053\n",
      "[epoch: 2, batches:  2000] loss: 0.987\n",
      "[epoch: 2, batches:  3000] loss: 0.963\n",
      "[epoch: 2, batches:  4000] loss: 0.948\n",
      "[epoch: 3, batches:  1000] loss: 0.911\n",
      "[epoch: 3, batches:  2000] loss: 0.897\n",
      "[epoch: 3, batches:  3000] loss: 0.899\n",
      "[epoch: 3, batches:  4000] loss: 0.866\n",
      "[epoch: 4, batches:  1000] loss: 0.856\n",
      "[epoch: 4, batches:  2000] loss: 0.832\n",
      "[epoch: 4, batches:  3000] loss: 0.832\n",
      "[epoch: 4, batches:  4000] loss: 0.808\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        if Flags['useGPU']:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())                \n",
    "        else:                \n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics every 500 mini-batches\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 1000 == 999:\n",
    "            print('[epoch: %d, batches: %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file: data/DRIVE/test/patches/02_test.npy\n"
     ]
    }
   ],
   "source": [
    "testset = DriveDatasetFromFile4Way(data_path=Dirs['test_data'], height=31, width=31, transform=transform)\n",
    "clss_test, class_counts_test = np.unique(testset.labels, return_counts=True)\n",
    "class_weights_test = 1.0/class_counts_test\n",
    "\n",
    "data_weights_test = np.array([class_weights_test[t] for t in testset.labels])\n",
    "data_weights_test = np.ones_like(testset.labels)\n",
    "\n",
    "second_min_class_count_test =  np.partition(class_counts_test, 1)[1]\n",
    "\n",
    "sampler_test = WeightedRandomSampler(data_weights_test, int(second_min_class_count_test))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=2, shuffle=False, num_workers=1, sampler=sampler_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 500 batches of test images: 63 %\n",
      "Accuracy of 1000 batches of test images: 63 %\n",
      "Accuracy of 1500 batches of test images: 63 %\n",
      "Accuracy of 2000 batches of test images: 64 %\n",
      "Accuracy of 2500 batches of test images: 64 %\n",
      "Accuracy of 3000 batches of test images: 64 %\n",
      "Accuracy of 3500 batches of test images: 64 %\n",
      "Accuracy of 4000 batches of test images: 64 %\n",
      "Accuracy of 4500 batches of test images: 64 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    \n",
    "    images, labels = data  \n",
    "    if Flags['useGPU']:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "    if i % 500 == 499:\n",
    "        print('Accuracy of %d batches of test images: %d %%' % (i+1, 100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check per-class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "i = 0\n",
    "for data in testloader:\n",
    "    \n",
    "    images, labels = data\n",
    "    \n",
    "    if Flags['useGPU']:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(2):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "    i = i+1\n",
    "    if i > 10000:\n",
    "        break\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
