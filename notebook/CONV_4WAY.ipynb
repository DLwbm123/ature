{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('/home/ak/Spring2018/ature')\n",
    "\n",
    "from neuralnet.utils.datasets import PatchesGenerator\n",
    "\n",
    "from utils import img_utils as imgutil\n",
    "from commons.IMAGE import Image\n",
    "from neuralnet.trainer import NNTrainer\n",
    "import neuralnet.utils.measurements as mnt\n",
    "import neuralnet.utils.data_utils as nndutils\n",
    "\n",
    "sep = os.sep\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define folders (create them if needed)\n",
    "Dirs = {}\n",
    "\n",
    "Dirs['train_data']= 'data'+sep+'DRIVE'+sep+'training'+sep +'patches'\n",
    "Dirs['test_data'] = 'data'+sep+'DRIVE'+sep+'test' +sep+ 'images1'\n",
    "Dirs['checkpoint']   = 'assests' +sep+ 'nnet_models'\n",
    "\n",
    "Dirs['data']      = 'data'+sep+'DRIVE'+sep+'test'\n",
    "Dirs['images']    = Dirs['data'] +sep+ 'images2'\n",
    "Dirs['mask']      = Dirs['data'] +sep+ 'mask'\n",
    "Dirs['truth']     = Dirs['data'] +sep+ '1st_manual'\n",
    "Dirs['segmented'] = Dirs['data'] +sep+ 'drive_segmented'\n",
    "\n",
    "for k, folder in Dirs.items():\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Set up execution flags\n",
    "Flags = {}\n",
    "Flags['useGPU'] = False\n",
    "\n",
    "\n",
    "num_classes = 4\n",
    "batch_size = 100\n",
    "epochs =1\n",
    "patch_size = 31\n",
    "\n",
    "classes = ('white', 'green', 'black', 'red')\n",
    "\n",
    "\n",
    "def get_mask_file(file_name): \n",
    "    return file_name.split('_')[0] + '_test_mask.gif'\n",
    "\n",
    "def get_ground_truth_file(file_name): \n",
    "    return file_name.split('_')[0] + '_manual1.gif'\n",
    "\n",
    "def get_segmented_file(file_name):\n",
    "    return file_name + '_SEG.PNG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, width, channels):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.width = width\n",
    "        \n",
    "    \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 2      \n",
    "        self.kern_padding = 2\n",
    "        self.mxp_kern_size = 2\n",
    "        self.mxp_stride = 2 \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        self.conv1 = nn.Conv2d(self.channels, 20, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 1      \n",
    "        self.kern_padding = 2\n",
    "        self.mxp_kern_size = 1\n",
    "        self.mxp_stride = 1 \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        self.conv2 = nn.Conv2d(20, 50, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 1      \n",
    "        self.kern_padding = 1\n",
    "        self.mxp_kern_size = 1\n",
    "        self.mxp_stride = 1 \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        self.conv3 = nn.Conv2d(50, 50, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.linearWidth = 50*int(self.width)*int(self.width)\n",
    "        self.fc1 = nn.Linear(self.linearWidth, 100)\n",
    "        self.fc2 = nn.Linear(100, 20)\n",
    "        self.fc3 = nn.Linear(20, 4)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, self.linearWidth)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def _update_output_size(self):       \n",
    "        temp = self.width\n",
    "        self.width = ((self.width - self.kern_size + 2 * self.kern_padding) / self.kern_stride) + 1\n",
    "        temp1 = self.width\n",
    "        self.width = ((self.width - self.mxp_kern_size)/self.mxp_stride) + 1\n",
    "        print('output width { ' + str(temp) + ' -conv-> ' + str(temp1) + ' -maxpool-> ' + str(self.width) + ' }')\n",
    "\n",
    "width = 31\n",
    "channels = 1\n",
    "net = Net(width, channels)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(40),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "trainset = PatchesGenerator(Dirs=Dirs, patch_size=patch_size, num_classes=num_classes, transform=transform,\n",
    "                 fget_mask=get_mask_file, fget_truth=get_ground_truth_file, fget_segmented=get_segmented_file) \n",
    "\n",
    "### Fix skewed classes by sampling based on class weights\n",
    "_, ccounts_train = np.unique(trainset.labels, return_counts=True)\n",
    "cweights_train = 1.0/ccounts_train\n",
    "dweights_train = np.array([ccounts_train[t] for t in trainset.labels])\n",
    "dweights_train = np.ones_like(trainset.labels)\n",
    "dmin =  np.partition(ccounts_train, 1)[1]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=3, \n",
    "                                          sampler=WeightedRandomSampler(dweights_train, int(2*dmin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "Dirs['images'] = Dirs['test_data']\n",
    "testset = PatchesGenerator(Dirs=Dirs, patch_size=patch_size, num_classes=num_classes, transform=transform,\n",
    "                 fget_mask=get_mask_file, fget_truth=get_ground_truth_file, fget_segmented=get_segmented_file) \n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = NNTrainer(model=net, checkpoint_dir=Dirs['checkpoint'], checkpoint_file='checkpoint4Way.nn.tar')\n",
    "# trainer.resume_from_checkpoint()\n",
    "trainer.train(optimizer=optimizer, dataloader=trainloader, epochs=epochs, use_gpu=Flags['useGPU'])\n",
    "# trainer.resume_from_checkpoint()\n",
    "acc, y_pred, y_true = trainer.test(dataloader=testloader, use_gpu=Flags['useGPU'], force_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc, y_pred, y_true = trainer.test(dataloader=testloader, use_gpu=Flags['useGPU'], force_checkpoint=True)\n",
    "mnt.plot_confusion_matrix(y_pred=y_pred, y_true=y_true, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check per-class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(num_classes))\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "for i in range(num_classes):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve throughout the image to generate segmented image based on trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        imgutil.whiten_image2d,\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "segset = PatchesGenerator(Dirs=Dirs, patch_size=patch_size, \n",
    "                             num_classes=num_classes, transform=transform,\n",
    "                             fget_mask=get_mask_file, fget_truth=get_ground_truth_file, \n",
    "                             fget_segmented=get_segmented_file, segment_mode=True) \n",
    "\n",
    "segloader = torch.utils.data.DataLoader(segset, batch_size=1, \n",
    "                                          shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = np.zeros_like(segset.images[0].working_arr)\n",
    "for ID, image, label in segloader:\n",
    "#     IMG.fromarray(np.array(image*255, np.uint8).reshape(31,31)).show()\n",
    "#     break\n",
    "    print(ID)\n",
    "    break\n",
    "    outputs = trainer.model(Variable(image))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if predicted[0]==3 or predicted[0] == 0:\n",
    "        seg[i, j] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG.fromarray(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
