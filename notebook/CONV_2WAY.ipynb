{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# NOTE: Aashis, you should use the training images to develop your algorithm.\n",
    "# The test images should be reserved for validating its performance afterwards\n",
    "# os.chdir('/home/ak/Spring2018/ature')\n",
    "os.chdir('/home/ak/Spring2018/ature')\n",
    "from utils import img_utils as imgutil\n",
    "\n",
    "sep = os.sep\n",
    "\n",
    "# Define folders (create them if needed)\n",
    "Dirs = {}\n",
    "Dirs['train_data']      = 'data'+sep+'DRIVE'+sep+'training'+sep +'patches'\n",
    "Dirs['test_data']      = 'data'+sep+'DRIVE'+sep+'test'+sep +'patches'\n",
    "\n",
    "# Set up execution flags\n",
    "Flags = {}\n",
    "Flags['useGPU'] = False\n",
    "\n",
    "classes = ('background', 'vessel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriveDataset(Dataset):\n",
    "    \n",
    "    # data_path should contain pickled numpy array of dimension N * (D+1)\n",
    "    # Where extra one dimension stores the correct lable among (0, 1, 2, 3)\n",
    "    \n",
    "    def __init__(self, data_path=None, height=None, width=None, transform=None):\n",
    "        \n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.data = None\n",
    "        for data_file in os.listdir(data_path):\n",
    "            \n",
    "            data_file = os.path.join(data_path, data_file)\n",
    "            print('Data file: ' + data_file)\n",
    "            if self.data is None:\n",
    "                self.data = np.load(data_file)\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data, np.load(data_file)), axis=0)\n",
    "            \n",
    "    \n",
    "        self.labels = self.data[:, self.height * self.width]\n",
    "        \n",
    "        self.labels[self.labels==0] = 1 # White (TP)\n",
    "        self.labels[self.labels==1] = 0 # Green (FP)\n",
    "        self.labels[self.labels==2] = 0 # Black (TN)\n",
    "        self.labels[self.labels==3] = 1 # Red (FN)\n",
    "        \n",
    "        self.labels = torch.from_numpy(self.labels) \n",
    "        self.data = self.data[:,0:self.height * self.width]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_arr = self.data[index].reshape(self.height, self.width)\n",
    "        img = IMG.fromarray(imgutil.whiten_image2d(img_arr))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img_tensor = self.transform(img)\n",
    "            \n",
    "        return (img_tensor, self.labels[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file: data/DRIVE/training/patches/25_training.npy\n",
      "Data file: data/DRIVE/training/patches/24_training.npy\n",
      "Data file: data/DRIVE/training/patches/21_training.npy\n",
      "Data file: data/DRIVE/training/patches/22_training.npy\n",
      "Data file: data/DRIVE/training/patches/23_training.npy\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "trainset = DriveDataset(data_path=Dirs['train_data'], height=31, width=31, transform=transform)\n",
    "clss, class_counts = np.unique(trainset.labels, return_counts=True)\n",
    "class_weights = 1.0/class_counts\n",
    "data_weights = np.array([class_weights[t] for t in trainset.labels]) \n",
    "second_min_class_count =  np.partition(class_counts, 1)[1]\n",
    "\n",
    "sampler = WeightedRandomSampler(data_weights, int(second_min_class_count))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=False, num_workers=1, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAAAAAA6xUnlAAABuklEQVR4nCXR0QEjWg1DQR3ZyVIh/XcBbK4lPl4LM/wbSeqLa52QW0NDtaP9cy+q5s97abF8klqdN8f+TQq63/jk3v4VuNQ8XLMG2G+l6/B29E7SybTbY8iP+TCR7BOtFNPCiqp8IJV63UNtGuyc9j/sojZJjJtRpcrWY7Tf1cUmPaPKFAytVunyGysRIyYnR9hIbVSWkeQCNJn+QHdqZVQ2+PWbByS8fCRUWZXY7HFePKEBIRL37Kodds8qEr6h4ACMdMW59fzGj/17zx9xD5paN6Py2ciJ8/Q/fCnsyaqcDnTjwwdvsaLhJ2H9I6gsM+kn91Hw9E5Wz4pR/NlB4p+LXsP2rsI8r3sbJTyhtKhXG43Ev+7Yrv67W/WuX14gYYb23kxfdz/vrOgj11R23Yu0SaU9C3lQfjeT8MzkRM4fdQnoWsq3v0pummCWitVc6WVGrdVKsis9wNo+qKf38zq33/aehObiZknRVXzvZqd/MShB04NNl7qizs/x3DVj/fyl6vrO2x/ak4SZ53n9kuYGvz+TF/FtC1Zu4Ou/Hbdi51hD/1IJcxY95uT1vR1xJ0kHLWrdCusF4P8/Kn33jkOp8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=31x31 at 0x7F52B842C828>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG.fromarray(trainset.data[3000].reshape(31,31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output width { conv: 16.0, maxpool: 8.0 }\n",
      "output width { conv: 6.0, maxpool: 6.0 }\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, width, channels):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.width = width\n",
    "        \n",
    "        \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 2      \n",
    "        self.kern_padding = 2\n",
    "        \n",
    "        self.mxp_kern_size = 2\n",
    "        self.mxp_stride = 2 \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.channels, 20, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 1      \n",
    "        self.kern_padding = 1\n",
    "        \n",
    "        self.mxp_kern_size = 1\n",
    "        self.mxp_stride = 1 \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(20, 50, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.linearWidth = 50*int(self.width)*int(self.width)\n",
    "        self.fc1 = nn.Linear(self.linearWidth, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "        self.fc3 = nn.Linear(10, 4)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.linearWidth)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def _update_output_size(self):       \n",
    "        self.width = ((self.width - self.kern_size + 2 * self.kern_padding) / self.kern_stride) + 1\n",
    "        temp = self.width\n",
    "        self.width = ((self.width - self.mxp_kern_size)/self.mxp_stride) + 1\n",
    "        print('output width { conv: ' + str(temp) + ', maxpool: ' + str(self.width) + ' }')\n",
    "\n",
    "width = 31\n",
    "channels = 1\n",
    "net = Net(width, channels)\n",
    "\n",
    "# Send network to the GPU, if requested\n",
    "if Flags['useGPU']:\n",
    "    net.cuda()\n",
    "\n",
    "# Define loss criterion\n",
    "# criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batches:  1000] loss: 0.819\n",
      "[epoch: 1, batches:  2000] loss: 0.705\n",
      "[epoch: 1, batches:  3000] loss: 0.702\n",
      "[epoch: 1, batches:  4000] loss: 0.702\n",
      "[epoch: 1, batches:  5000] loss: 0.700\n",
      "[epoch: 1, batches:  6000] loss: 0.698\n",
      "[epoch: 1, batches:  7000] loss: 0.696\n",
      "[epoch: 1, batches:  8000] loss: 0.699\n",
      "[epoch: 1, batches:  9000] loss: 0.699\n",
      "[epoch: 1, batches: 10000] loss: 0.698\n",
      "[epoch: 1, batches: 11000] loss: 0.696\n",
      "[epoch: 1, batches: 12000] loss: 0.698\n",
      "[epoch: 1, batches: 13000] loss: 0.699\n",
      "[epoch: 1, batches: 14000] loss: 0.699\n",
      "[epoch: 1, batches: 15000] loss: 0.696\n",
      "[epoch: 1, batches: 16000] loss: 0.694\n",
      "[epoch: 1, batches: 17000] loss: 0.698\n",
      "[epoch: 1, batches: 18000] loss: 0.694\n",
      "[epoch: 1, batches: 19000] loss: 0.695\n",
      "[epoch: 1, batches: 20000] loss: 0.692\n",
      "[epoch: 1, batches: 21000] loss: 0.691\n",
      "[epoch: 1, batches: 22000] loss: 0.688\n",
      "[epoch: 1, batches: 23000] loss: 0.681\n",
      "[epoch: 1, batches: 24000] loss: 0.676\n",
      "[epoch: 1, batches: 25000] loss: 0.664\n",
      "[epoch: 1, batches: 26000] loss: 0.648\n",
      "[epoch: 1, batches: 27000] loss: 0.629\n",
      "[epoch: 1, batches: 28000] loss: 0.625\n",
      "[epoch: 1, batches: 29000] loss: 0.609\n",
      "[epoch: 1, batches: 30000] loss: 0.589\n",
      "[epoch: 1, batches: 31000] loss: 0.589\n",
      "[epoch: 1, batches: 32000] loss: 0.577\n",
      "[epoch: 1, batches: 33000] loss: 0.572\n",
      "[epoch: 1, batches: 34000] loss: 0.567\n",
      "[epoch: 1, batches: 35000] loss: 0.563\n",
      "[epoch: 1, batches: 36000] loss: 0.544\n",
      "[epoch: 1, batches: 37000] loss: 0.550\n",
      "[epoch: 1, batches: 38000] loss: 0.529\n",
      "[epoch: 1, batches: 39000] loss: 0.515\n",
      "[epoch: 1, batches: 40000] loss: 0.522\n",
      "[epoch: 1, batches: 41000] loss: 0.528\n",
      "[epoch: 1, batches: 42000] loss: 0.527\n",
      "[epoch: 1, batches: 43000] loss: 0.519\n",
      "[epoch: 1, batches: 44000] loss: 0.514\n",
      "[epoch: 1, batches: 45000] loss: 0.526\n",
      "[epoch: 1, batches: 46000] loss: 0.495\n",
      "[epoch: 1, batches: 47000] loss: 0.498\n",
      "[epoch: 1, batches: 48000] loss: 0.487\n",
      "[epoch: 1, batches: 49000] loss: 0.492\n",
      "[epoch: 1, batches: 50000] loss: 0.497\n",
      "[epoch: 1, batches: 51000] loss: 0.500\n",
      "[epoch: 1, batches: 52000] loss: 0.496\n",
      "[epoch: 1, batches: 53000] loss: 0.489\n",
      "[epoch: 1, batches: 54000] loss: 0.473\n",
      "[epoch: 1, batches: 55000] loss: 0.488\n",
      "[epoch: 1, batches: 56000] loss: 0.481\n",
      "[epoch: 1, batches: 57000] loss: 0.490\n",
      "[epoch: 1, batches: 58000] loss: 0.477\n",
      "[epoch: 1, batches: 59000] loss: 0.497\n",
      "[epoch: 1, batches: 60000] loss: 0.472\n",
      "[epoch: 1, batches: 61000] loss: 0.491\n",
      "[epoch: 1, batches: 62000] loss: 0.466\n",
      "[epoch: 1, batches: 63000] loss: 0.480\n",
      "[epoch: 1, batches: 64000] loss: 0.458\n",
      "[epoch: 1, batches: 65000] loss: 0.451\n",
      "[epoch: 1, batches: 66000] loss: 0.464\n",
      "[epoch: 1, batches: 67000] loss: 0.444\n",
      "[epoch: 1, batches: 68000] loss: 0.464\n",
      "[epoch: 1, batches: 69000] loss: 0.469\n",
      "[epoch: 1, batches: 70000] loss: 0.453\n",
      "[epoch: 1, batches: 71000] loss: 0.463\n",
      "[epoch: 1, batches: 72000] loss: 0.455\n",
      "[epoch: 1, batches: 73000] loss: 0.455\n",
      "[epoch: 1, batches: 74000] loss: 0.447\n",
      "[epoch: 1, batches: 75000] loss: 0.457\n",
      "[epoch: 1, batches: 76000] loss: 0.454\n",
      "[epoch: 1, batches: 77000] loss: 0.453\n",
      "[epoch: 1, batches: 78000] loss: 0.438\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        if Flags['useGPU']:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())                \n",
    "        else:                \n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics every 500 mini-batches\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 1000 == 999:\n",
    "            print('[epoch: %d, batches: %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = DriveDataset(data_path=Dirs['test_data'], height=31, width=31, transform=transform)\n",
    "clss_test, class_counts_test = np.unique(testset.labels, return_counts=True)\n",
    "class_weights_test = 1.0/class_counts_test\n",
    "\n",
    "data_weights_test = np.array([class_weights_test[t] for t in testset.labels])\n",
    "data_weights_test = np.ones_like(testset.labels)\n",
    "\n",
    "second_min_class_count_test =  np.partition(class_counts_test, 1)[1]\n",
    "\n",
    "sampler_test = WeightedRandomSampler(data_weights_test, int(second_min_class_count_test))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=2, shuffle=False, num_workers=1, sampler=sampler_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    \n",
    "    images, labels = data  \n",
    "    if Flags['useGPU']:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "    if i % 500 == 499:\n",
    "        print('Accuracy of %d batches of test images: %d %%' % (i+1, 100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check per-class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "i = 0\n",
    "for data in testloader:\n",
    "    \n",
    "    images, labels = data\n",
    "    \n",
    "    if Flags['useGPU']:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(2):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "    i = i+1\n",
    "    if i > 10000:\n",
    "        break\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
