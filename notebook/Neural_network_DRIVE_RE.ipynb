{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "\n",
    "# Other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# NOTE: Aashis, you should use the training images to develop your algorithm.\n",
    "# The test images should be reserved for validating its performance afterwards\n",
    "# os.chdir('/home/ak/Spring2018/ature')\n",
    "os.chdir('/home/rolando/Research/ature5/')\n",
    "\n",
    "sep = os.sep\n",
    "\n",
    "# Define folders (create them if needed)\n",
    "Dirs = {}\n",
    "Dirs['data']      = 'data'+sep+'DRIVE'+sep+'test'+sep +'patches'\n",
    "\n",
    "# Set up execution flags\n",
    "Flags = {}\n",
    "Flags['useGPU'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriveDataset(Dataset):\n",
    "    \n",
    "    ## INPUT\n",
    "    # data_path should contain pickled numpy array of dimension N * (D+1)\n",
    "    # Where extra one dimension stores the correct lable among (0, 1, 2, 3)\n",
    "    \n",
    "    ## self.labels contains one-hot encoding\n",
    "    ## self.target contains true labels (single value)\n",
    "    \n",
    "    def __init__(self, data_path=None, height=None, width=None, transform=None):\n",
    "        \n",
    "        self.data = None\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.transform = transform\n",
    "        \n",
    "        for data_file in os.listdir(data_path):\n",
    "            \n",
    "            data_file = os.path.join(data_path, data_file)\n",
    "            print('Data file: ' + data_file)\n",
    "            if self.data is None:\n",
    "                self.data = np.load(data_file)\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data, np.load(data_file)), axis=0)\n",
    "    \n",
    "        self.data_len = self.data.shape[0]\n",
    "        \n",
    "        self.labels = np.zeros((self.data_len, 4))\n",
    "        self.target = self.data[:, self.height * self.width] \n",
    "        self.labels[range(self.data_len), self.target] = 1\n",
    "        \n",
    "        self.data = self.data[:,0:self.height * self.width]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        a_data = self.data[index]\n",
    "        img_arr = a_data.reshape(self.height, self.width)\n",
    "        img = IMG.fromarray(img_arr)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img_tensor = self.transform(img)\n",
    "#             \n",
    "#         return (img_tensor, torch.LongTensor(self.target[index]))\n",
    "        return (img_tensor, torch.from_numpy(self.labels[index])) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file: data/DRIVE/test/patches/17_test.npy\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data_path = Dirs['data']\n",
    "\n",
    "trainset = DriveDataset(data_path=data_path, height=51, width=51, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root=Dirs['data'], train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('white', 'green', 'black', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADMAAAAzCAAAAAAfym/2AAAEaElEQVR4nC2Wsa4kSQ4Dg1TWm3XWuN++D97b6UqRZ/QAsiVBJAPSf/tk5Tzem84oqp014n31A2Z3RrHYys7JVE/LbTdHF3cNE571YzSK0VbFOrnMcZWFMlVHIrXaF33ASksFEBO/snLgc3wrViLI3tIwJzeega3cYFJzsnNKL7glEnQLbUmH26pvjxKbHWjQniSyG01XR4tIJdRqquTbgMYuaqrTaFw9QKG19s+8qZ1bK8gNV8LFPla7UpugQFNoQzFbCbG2DGp2mR4Eoq9GtABXgMUdVia3fee47eacKucaNVHfX3dxxdBKhGV019R2Lz3n3cg+22kYolduWXVaFcd9pelWXhBX7invMUsphRFUBqgsFUxRKmrBIsDHqBCIAqPWd6PHqTecuf3lz9WZTZDAR6QIiRZJ6geZ3tTZKsu1n33vYKjUE766U1uEMtNLw4l+uD7ZtU5LqSicIpmNZpCUJr358ZJ+Ve7oj/MiATookk6YRlLa08Nhd9zoULWnaeFrKc53STcR9vZ45byaJ2xF193M8e6IAvf0Fi3ePE9fTyoVqRuJ14+DZth3nGspHFBqVb4XSeLaIPXkm6STja1HXiL7nlTeV8ftarYi1+7iwkyqwve2DSCdM1fywPi3bJee68mCchk82zbISIR0jmaplF3pa/Ed58rVUCeijTxulUB96rg5D9cDKRqS4+5a8J3xmN35el49srdKOlcq0TQVtFCQ7CXrpy+CrHU+wbN18uzleHlnNkONXPotARUS5pwu87M92meyVtSL72KbzdP+dffkxUM3fc+5+iIPzkanowKy1cLT5NnwSju0Prs5n3PyGyH5Vo5MW2mgvEiPipC7JgL3fH7HI6OZuqBqATVmq96NViainlzMuWnryfbMw0glRXY0aJa9cI6srRjSc3qNyXLYW7U1UAkOY3480fRLs2K1p8eVp9svSzCiUtlXx7o1J8dHjbBgT5qGVvqZdkfCqdSwp9eX/fno8fNLtyOinrNfUKPdi246duQuU467Z7LqP+cZ+4MP5z/bvVLYVLc6za68LVdQz/z6ucX7aXfmaM4tM7Ye7e/4vvVWo72Qwq6TZ+Bnm7V1ozO0i8fag5+/1K0PCdsmn8JGjfvoGXKHY4LY/eBVdA5JwebJDc/kf7cfrfqvZ4zcw4JoLdGvcFCAj/kR/vy9N7zLJb+/7LW8xahXLuoiC5GfbuN5mB81C9wld+9hanprnWKqn6WQ9EEim9rZQ9SfcTZ7QOvSCi/OjaVWj9/SKmIXfTJrQL+4h4S8gnd+/jCkUou0jNqJU0Z9gK3013njdqDuTYeREnUxcZ1LezkMHUzgn2OlUlop9aQ8n9pO9UMrVrEVjb8hkc+K0WpUp2OXT0SRfDYXABviZ1mR6gRFImL9vXftlEoprQVcVX2plbwc4625SMiQPNlKjVNN6PFezBVI8tMD0i5VT9fC/nem3/8nEsrv+QMmt8TtKd+g0LpVX6wStDpboWcDttiO0n3n/zHmHvho3M8OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=51x51 at 0x7F73F418B1D0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG.fromarray(trainset.data[10].reshape(51,51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(Net, self).__init__()\n",
    "        nChannels = 1\n",
    "        convKernelSize = 5\n",
    "        convStride = 1\n",
    "        mpKernelSize = 2\n",
    "        mpStride = 2        \n",
    "        padding = 0\n",
    "        dilation = 1\n",
    "\n",
    "        self.pool = nn.MaxPool2d(mpStride)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(nChannels, 6, convKernelSize)\n",
    "        width = self.output_size(width,convKernelSize,padding,dilation,\n",
    "            mpKernelSize,mpStride)\n",
    "        self.conv2 = nn.Conv2d(6, 16, convKernelSize)\n",
    "        width = self.output_size(width,convKernelSize,padding,dilation,\n",
    "            mpKernelSize,mpStride)\n",
    "\n",
    "        self.linearWidth = 16*int(width)*int(width)\n",
    "        self.fc1 = nn.Linear(self.linearWidth, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.linearWidth)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    # Output size of a convolution, followed by max pooling operation\n",
    "    def output_size(self,inputSize,convKernelSize,padding,dilation,\n",
    "                    kernelSize,stride):\n",
    "        return np.floor(((inputSize - 2*np.floor(convKernelSize/2)) + \n",
    "                        2*padding - dilation*(kernelSize-1) - 1) / stride + 1)\n",
    "\n",
    "\n",
    "width = 51\n",
    "net = Net(width)\n",
    "\n",
    "# Send network to the GPU, if requested\n",
    "if Flags['useGPU']:\n",
    "    net.cuda()\n",
    "\n",
    "# Define loss criterion\n",
    "# criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.599\n",
      "[1,  4000] loss: 0.588\n",
      "[1,  6000] loss: 0.560\n",
      "[1,  8000] loss: 0.556\n",
      "[1, 10000] loss: 0.543\n",
      "[1, 12000] loss: 0.545\n",
      "[1, 14000] loss: 0.581\n",
      "[1, 16000] loss: 0.590\n",
      "[1, 18000] loss: 0.561\n",
      "[1, 20000] loss: 0.590\n",
      "[1, 22000] loss: 0.572\n",
      "[1, 24000] loss: 0.547\n",
      "[1, 26000] loss: 0.575\n",
      "[1, 28000] loss: 0.560\n",
      "[1, 30000] loss: 0.535\n",
      "[1, 32000] loss: 0.576\n",
      "[1, 34000] loss: 0.530\n",
      "[1, 36000] loss: 0.534\n",
      "[1, 38000] loss: 0.559\n",
      "[1, 40000] loss: 0.577\n",
      "[1, 42000] loss: 0.591\n",
      "[1, 44000] loss: 0.553\n",
      "[1, 46000] loss: 0.547\n",
      "[1, 48000] loss: 0.571\n",
      "[1, 50000] loss: 0.570\n",
      "[1, 52000] loss: 0.565\n",
      "[1, 54000] loss: 0.553\n",
      "[1, 56000] loss: 0.574\n",
      "[1, 58000] loss: 0.571\n",
      "[1, 60000] loss: 0.505\n",
      "[1, 62000] loss: 0.450\n",
      "[1, 64000] loss: 0.424\n",
      "[1, 66000] loss: 0.461\n",
      "[1, 68000] loss: 0.559\n",
      "[1, 70000] loss: 0.405\n",
      "[1, 72000] loss: 0.380\n",
      "[1, 74000] loss: 0.359\n",
      "[1, 76000] loss: 0.420\n",
      "[1, 78000] loss: 0.402\n",
      "[1, 80000] loss: 0.331\n",
      "[1, 82000] loss: 0.323\n",
      "[1, 84000] loss: 0.356\n",
      "[2,  2000] loss: 0.488\n",
      "[2,  4000] loss: 0.360\n",
      "[2,  6000] loss: 0.319\n",
      "[2,  8000] loss: 0.302\n",
      "[2, 10000] loss: 0.309\n",
      "[2, 12000] loss: 0.298\n",
      "[2, 14000] loss: 0.290\n",
      "[2, 16000] loss: 0.287\n",
      "[2, 18000] loss: 0.283\n",
      "[2, 20000] loss: 0.285\n",
      "[2, 22000] loss: 0.279\n",
      "[2, 24000] loss: 0.282\n",
      "[2, 26000] loss: 0.273\n",
      "[2, 28000] loss: 0.289\n",
      "[2, 30000] loss: 0.296\n",
      "[2, 32000] loss: 0.256\n",
      "[2, 34000] loss: 0.282\n",
      "[2, 36000] loss: 0.294\n",
      "[2, 38000] loss: 0.286\n",
      "[2, 40000] loss: 0.262\n",
      "[2, 42000] loss: 0.240\n",
      "[2, 44000] loss: 0.254\n",
      "[2, 46000] loss: 0.269\n",
      "[2, 48000] loss: 0.240\n",
      "[2, 50000] loss: 0.297\n",
      "[2, 52000] loss: 0.274\n",
      "[2, 54000] loss: 0.246\n",
      "[2, 56000] loss: 0.248\n",
      "[2, 58000] loss: 0.257\n",
      "[2, 60000] loss: 0.239\n",
      "[2, 62000] loss: 0.264\n",
      "[2, 64000] loss: 0.249\n",
      "[2, 66000] loss: 0.235\n",
      "[2, 68000] loss: 0.241\n",
      "[2, 70000] loss: 0.247\n",
      "[2, 72000] loss: 0.255\n",
      "[2, 74000] loss: 0.225\n",
      "[2, 76000] loss: 0.231\n",
      "[2, 78000] loss: 0.216\n",
      "[2, 80000] loss: 0.232\n",
      "[2, 82000] loss: 0.245\n",
      "[2, 84000] loss: 0.234\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        labels = labels.long()\n",
    "        labels = torch.max(labels,1)[1]\n",
    "\n",
    "\n",
    "        if Flags['useGPU']:\n",
    "            # Wrap them in Variable (CPU version)\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())                \n",
    "        else:                \n",
    "            # Wrap them in Variable (GPU version)\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "#         print(inputs, labels)\n",
    "#         break\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-10c7a3033fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFlags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'useGPU'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data        \n",
    "    if Flags['useGPU']:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check per-class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    if Flags['useGPU']:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169795, 2601)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
