{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import PIL.Image as IMG\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# NOTE: Aashis, you should use the training images to develop your algorithm.\n",
    "# The test images should be reserved for validating its performance afterwards\n",
    "# os.chdir('/home/ak/Spring2018/ature')\n",
    "os.chdir('/home/ak/Spring2018/ature')\n",
    "\n",
    "sep = os.sep\n",
    "\n",
    "# Define folders (create them if needed)\n",
    "Dirs = {}\n",
    "Dirs['train_data']      = 'data'+sep+'DRIVE'+sep+'training'+sep +'patches'\n",
    "Dirs['test_data']      = 'data'+sep+'DRIVE'+sep+'test'+sep +'patches'\n",
    "\n",
    "# Set up execution flags\n",
    "Flags = {}\n",
    "Flags['useGPU'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriveDataset(Dataset):\n",
    "    \n",
    "    # data_path should contain pickled numpy array of dimension N * (D+1)\n",
    "    # Where extra one dimension stores the correct lable among (0, 1, 2, 3)\n",
    "    \n",
    "    def __init__(self, data_path=None, height=None, width=None, transform=None):\n",
    "        \n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.data = None\n",
    "        for data_file in os.listdir(data_path):\n",
    "            \n",
    "            data_file = os.path.join(data_path, data_file)\n",
    "            print('Data file: ' + data_file)\n",
    "            if self.data is None:\n",
    "                self.data = np.load(data_file)\n",
    "            else:\n",
    "                self.data = np.concatenate((self.data, np.load(data_file)), axis=0)\n",
    "            \n",
    "    \n",
    "        self.labels = torch.from_numpy(self.data[:, self.height * self.width]) \n",
    "        self.data = self.data[:,0:self.height * self.width]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        a_data = self.data[index]\n",
    "        img_arr = a_data.reshape(self.height, self.width)\n",
    "        img = IMG.fromarray(img_arr)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img_tensor = self.transform(img)\n",
    "            \n",
    "        return (img_tensor, self.labels[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file: data/DRIVE/training/patches/25_training.npy\n",
      "Data file: data/DRIVE/training/patches/24_training.npy\n",
      "Data file: data/DRIVE/training/patches/21_training.npy\n",
      "Data file: data/DRIVE/training/patches/22_training.npy\n",
      "Data file: data/DRIVE/training/patches/23_training.npy\n",
      "Data file: data/DRIVE/test/patches/02_test.npy\n",
      "Data file: data/DRIVE/test/patches/01_test.npy\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = DriveDataset(data_path=Dirs['train_data'], height=31, width=31, transform=transform)\n",
    "clss, class_counts = np.unique(trainset.labels, return_counts=True)\n",
    "class_weights = 1.0/class_counts\n",
    "data_weights = np.array([class_weights[t] for t in trainset.labels]) \n",
    "second_min_class_count =  np.partition(class_counts, 1)[1]\n",
    "\n",
    "sampler = WeightedRandomSampler(data_weights, int(second_min_class_count))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=3, shuffle=False, num_workers=1, sampler=sampler)\n",
    "\n",
    "testset = DriveDataset(data_path=Dirs['test_data'], height=31, width=31, transform=transform)\n",
    "clss_test, class_counts_test = np.unique(testset.labels, return_counts=True)\n",
    "class_weights_test = 1.0/class_counts_test\n",
    "data_weights_test = np.array([class_weights[t] for t in trainset.labels]) \n",
    "second_min_class_count_test =  np.partition(class_counts_test, 1)[1]\n",
    "\n",
    "sampler_test = WeightedRandomSampler(data_weights, int(second_min_class_count))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=2, shuffle=False, num_workers=1, sampler=sampler_test)\n",
    "\n",
    "classes = ('white', 'green', 'black', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAAAAAA6xUnlAAABxklEQVR4nCXSQY7cUAwDUZKS3YMcL/c/SMb+UmXRWwK1eAD9t6OhLCHkxXOUConBfc2JCxfSckYt5eoslqReWwSDwqvaX7pHRtGaloSnNKM8Pvi6/d7CEbba67ve2V3DOuzpNrZAnq4SvxxdOStF1fMbUwnJST8a7okPcktXJqlThmIz7WV/+92+qvt9TqrOG4ElbfVZhalWin/E+wZve9/jshoQnTtavhH2ateOpR5XlatZVAwuR4TFkrs/bVRh1npKijEjibWLjqT10LVbQCLRIpcW3u5jc5JFtgVHV69hwKveghYIma4ZB+wlgmwLWByEc6buMzkW2Cmpn6/iVZunwkNyj2wtxP3D7lDXCn9OgLW9GztGPXLr7NfoN/c7kmFiAd3sfL+EbO9kbVsrrZQgd4O+hLvXLXckyBW3AipnLXZXRu3ade163YNSZxRvSdL42pxjP5vdu7XRxNGQPlL7sXa2rmhQbwc2psSraC2dwVpJqEvHHZAkVsjodRuTetOxZEE0+bDER/1d2EzjcJRIXYoRcpAizUads8llXnX2VYF2rXixn5926Ivja0XzINkXWGD9md4qixo+mTefHcnLGsh16j8sJogkBVH7DgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=31x31 at 0x7F44796F2208>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG.fromarray(trainset.data[10000].reshape(31,31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944541, 961)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output width { conv: 16.0, maxpool:8.0 }\n",
      "output width { conv: 6.0, maxpool:6.0 }\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, width, channels):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.width = width\n",
    "        \n",
    "        \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 2      \n",
    "        self.kern_padding = 2\n",
    "        \n",
    "        self.mxp_kern_size = 2\n",
    "        self.mxp_stride = 2 \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.channels, 20, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        self.kern_size = 5\n",
    "        self.kern_stride = 1      \n",
    "        self.kern_padding = 1\n",
    "        \n",
    "        self.mxp_kern_size = 1\n",
    "        self.mxp_stride = 1 \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=self.mxp_kern_size, stride=self.mxp_stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(20, 50, self.kern_size, \n",
    "                               stride=self.kern_stride, padding=self.kern_padding)\n",
    "        self._update_output_size()\n",
    "        \n",
    "        \n",
    "        self.linearWidth = 50*int(self.width)*int(self.width)\n",
    "        self.fc1 = nn.Linear(self.linearWidth, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "        self.fc3 = nn.Linear(10, 4)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.linearWidth)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def _update_output_size(self):       \n",
    "        self.width = ((self.width - self.kern_size + 2 * self.kern_padding) / self.kern_stride) + 1\n",
    "        temp = self.width\n",
    "        self.width = ((self.width - self.mxp_kern_size)/self.mxp_stride) + 1\n",
    "        print('output width { conv: ' + str(temp) + ', maxpool:' + str(self.width) + ' }')\n",
    "\n",
    "width = 31\n",
    "channels = 1\n",
    "net = Net(width, channels)\n",
    "\n",
    "# Send network to the GPU, if requested\n",
    "if Flags['useGPU']:\n",
    "    net.cuda()\n",
    "\n",
    "# Define loss criterion\n",
    "# criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.395\n",
      "[1,  2000] loss: 1.398\n",
      "[1,  3000] loss: 1.395\n",
      "[1,  4000] loss: 1.396\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        if Flags['useGPU']:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())                \n",
    "        else:                \n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics every 500 mini-batches\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 1000 == 999:\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 500 test images: 67 %\n",
      "Accuracy of 1000 test images: 66 %\n",
      "Accuracy of 1500 test images: 67 %\n",
      "Accuracy of 2000 test images: 67 %\n",
      "Accuracy of 2500 test images: 67 %\n",
      "Accuracy of 3000 test images: 66 %\n",
      "Accuracy of 3500 test images: 66 %\n",
      "Accuracy of 4000 test images: 66 %\n",
      "Accuracy of 4500 test images: 67 %\n",
      "Accuracy of 5000 test images: 67 %\n",
      "Accuracy of 5500 test images: 67 %\n",
      "Accuracy of 6000 test images: 67 %\n",
      "Accuracy of 6500 test images: 67 %\n",
      "Accuracy of 7000 test images: 67 %\n",
      "Accuracy of 7500 test images: 67 %\n",
      "Accuracy of 8000 test images: 67 %\n",
      "Accuracy of 8500 test images: 67 %\n",
      "Accuracy of 9000 test images: 67 %\n",
      "Accuracy of 9500 test images: 67 %\n",
      "Accuracy of 10000 test images: 67 %\n",
      "Accuracy of 10500 test images: 67 %\n",
      "Accuracy of 11000 test images: 67 %\n",
      "Accuracy of 11500 test images: 67 %\n",
      "Accuracy of 12000 test images: 67 %\n",
      "Accuracy of 12500 test images: 67 %\n",
      "Accuracy of 13000 test images: 67 %\n",
      "Accuracy of 13500 test images: 67 %\n",
      "Accuracy of 14000 test images: 67 %\n",
      "Accuracy of 14500 test images: 67 %\n",
      "Accuracy of 15000 test images: 67 %\n",
      "Accuracy of 15500 test images: 67 %\n",
      "Accuracy of 16000 test images: 67 %\n",
      "Accuracy of 16500 test images: 67 %\n",
      "Accuracy of 17000 test images: 67 %\n",
      "Accuracy of 17500 test images: 67 %\n",
      "Accuracy of 18000 test images: 67 %\n",
      "Accuracy of 18500 test images: 67 %\n",
      "Accuracy of 19000 test images: 67 %\n",
      "Accuracy of 19500 test images: 67 %\n",
      "Accuracy of 20000 test images: 67 %\n",
      "Accuracy of 20500 test images: 67 %\n",
      "Accuracy of 21000 test images: 67 %\n",
      "Accuracy of 21500 test images: 67 %\n",
      "Accuracy of 22000 test images: 67 %\n",
      "Accuracy of 22500 test images: 67 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    \n",
    "    images, labels = data  \n",
    "    if Flags['useGPU']:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "    if i % 500 == 499:\n",
    "        print('Accuracy of %d test images: %d %%' % (i+1, 100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check per-class performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of white : 82 %\n",
      "Accuracy of green :  5 %\n",
      "Accuracy of black : 98 %\n",
      "Accuracy of   red :  5 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "i = 0\n",
    "for data in testloader:\n",
    "    \n",
    "    images, labels = data\n",
    "    \n",
    "    if Flags['useGPU']:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(2):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "    i = i+1\n",
    "    if i > 10000:\n",
    "        break\n",
    "\n",
    "for i in range(4):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45368"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_min_class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ature_env",
   "language": "python",
   "name": "ature_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
